---
title: "DADA2 18S workflow"
author: Chris Trivedi
output: html_notebook
---

<!-- This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code.  -->

<!-- Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*.  -->

<!-- ```{r} -->
<!-- plot(cars) -->
<!-- ``` -->

<!-- Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*. -->

<!-- When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).  -->

<!-- The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed. -->

setwd("~/Working/Data/IS_18_ITS/18S")

Load dada2
```
library(dada2); packageVersion("dada2")
```

Unzip any fastq files that still need it
```
gzip -d -k *.gz # -d is decompress, -k keeps original files, *.gz will run on any gzipped files within the current directory
```

Remove unnecessary gzip files that may take up space
```
rm *.gz -v
```


### Set the path to your unzipped data and list the present files
```{r}
path <- "/home/ctrivedi/Working/Data/IS_18_ITS/18S/" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```

### Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
```{r}
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

### Inspect quality profiles
```{r}
plotQualityProfile(fnFs[1:2]) #Forward reads
plotQualityProfile(fnRs[1:2]) #Reverse reads
```

Based on our profiles it looks like we can cut the Forward reads at approximately 270 and the Reverse reads at 190 - Unfortunately this is pretty typical for Illumina 2x300 v3 chemistry kit.

### Place filtered files in filtered/ subdirectory
```
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
```

### Trim
```
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(280,200),
              maxN=0, maxEE=c(2,5), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
# You can specify the number of threads to utilize when using a shared resource
```
<!-- By increasing our maxEE from 2 to 7 we saved almost 50k sequences -->
Changed this to 2,5

### DADA2 uses a parametric error model and every amplicon set has different error rates - we can create a model of our data and use this down the line to help evaluate our reads.

```
errF <- learnErrors(filtFs, multithread=8, verbose=TRUE)
errR <- learnErrors(filtRs, multithread=4, verbose=TRUE)
```

Visualize the error rates as a sanity check
```
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```

### Dereplication
```
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

### Sample Inference
```
dadaFs <- dada(derepFs, err=errF, multithread=8)
dadaRs <- dada(derepRs, err=errR, multithread=8)
dadaFs[[1]]
dadaRs[[1]]
```

### Merge paired reads
```
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

### Construct sequence table
```
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

### Removing chimeras
```
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
```

### Track reads through the pipeline
```
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

### Assigning taxonomy
```
taxa <- assignTaxonomy(seqtab.nochim, "/opt/databases/DADA2/dada2_training_data/silva_nr_v132_train_set.fa.gz", multithread=8)
```

### Can also attempt to call species level on exact matches
```
taxa <- addSpecies(taxa, "/opt/databases/DADA2/dada2_training_data/silva_species_assignment_v132.fa.gz")
```

<!-- ### Need to try IDTAXA via the DECIPHER package - which apparently reports better classification performance. -->

### Inspect Assignments
```
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

### Load packages for downstream analysis
```
library(phangorn) # These are part of BStamps' workflow
library(DECIPHER)

library(phyloseq); packageVersion("phyloseq")
library(ggplot2); packageVersion("ggplot2")
```

theme_set(theme_bw())

### From Gloria Iriarte's workflow
```
seqtab.nochim1 <- seqtab.nochim
colnames(seqtab.nochim1) <- paste0("SV", 1:ncol(seqtab.nochim1))

# Then added taxonomy to the original seqtab.nochim
taxtab <- assignTaxonomy(seqtab.nochim, "/opt/databases/DADA2/dada2_training_data/silva_nr_v132_train_set.fa.gz", minBoot = 50, tryRC = TRUE, outputBootstraps = FALSE, taxLevels = c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species"), multithread = 8, verbose = TRUE)


                           
# change the names again (the order was the same as in seqtab.nochim)
taxtab1 <- taxtab
rownames(taxtab1) <- paste0("SV", 1:nrow(taxtab1))
```

Install and load [mctools](https://github.com/leffj/mctoolsr) and seqinr for this next part
```
install.packages("devtools")
devtools::install_github("leffj/mctoolsr")
install.packages("seqinr")

library("mctoolsr")
library("seqinr")
library("ips")
```

### Export SV table and sequences
```
export_taxa_table_and_seqs = function(seqtab.nochim, file_seqtab, file_seqs) {
      seqtab.t = as.data.frame(t(seqtab.nochim))
      seqs = row.names(seqtab.t)
      row.names(seqtab.t) = paste0("SV", 1:nrow(seqtab.t))
      outlist = list(data_loaded = seqtab.t)
      mctoolsr::export_taxa_table(outlist, file_seqtab)
      seqs = as.list(seqs)
      seqinr::write.fasta(seqs, row.names(seqtab.t), file_seqs)
    }

export_taxa_table_and_seqs(seqtab.nochim,"SV_table.txt", "SV_seqs.fa")
```

### Now lets load the SV `.fasta` file and align the seqs using DECIPHER
```
fas <- "SV_seqs.fa"
seqs <- readDNAStringSet(fas)

seqs <- OrientNucleotides(seqs) ## I found duplicated SVs after I reoriented the sequences

alignment <- AlignSeqs(seqs, anchor=NA)
  

phang.align <- phyDat(as(alignment, "matrix"), type="DNA")
write.phyDat(phang.align, file="alignment.fasta", format="fasta")
  
phang.align <- read.dna("alignment.fasta",format="fasta",as.matrix=TRUE)
alignment.rax.gtr <- raxml(phang.align,
                             m="GTRGAMMAIX", # model
                             f="a", # best tree and bootstrap
                             p=1234, # random number seed
                             x=2345, # random seed for rapid bootstrapping
                             N=100, # number of bootstrap replicates
                             file="alignment", # name of output files
                             exec="raxmlHPC-PTHREADS-SSE3", # name of executable
                             threads=16
  )
```

# Then you incorporate everything to your phyloseq object:
```
tree <- read_tree("RAxML_bipartitionsBranchLabels.alignment")
metadata_path <- "/home/ctrivedi/Working/20190321_IS18_18S_Map.csv"
samdf <- read.csv(metadata_path, header=TRUE)
all(rownames(seqtab.nochim1) %in% samdf$X.SampleID)
rownames(samdf) <- samdf$X.SampleID
  
ps_18S <- phyloseq(tax_table(taxtab1), sample_data(samdf),otu_table(seqtab.nochim1, taxa_are_rows = FALSE), phy_tree(tree))

saveRDS(ps_18S, file="ps_18S.rds")
```

### Output files for analysis w/ vegan package

write.csv(taxtab1, "taxa_table.csv")

write.csv(seqtab.nochim1, "seqtab_nochim1.csv")

### For Matthias 17 Sep, 2019 ###

### Bar charts across all samples on the Phylum level
```{r, warning=FALSE}
plot_bar(ps_18S, "Sample_sheet_name", fill = "Phylum") + theme(legend.position="bottom")
```

### Observing general richness across sites
```{r}
plot_richness(ps_18S, measures = c("Observed", "Chao1", "Shannon", "Simpson"), x= "Site")
```


### Again, split by Month and replicates for each site
```{r, warning=FALSE}
plot_bar(ps_18S, "Type", fill = "Phylum", facet_grid = "Site") + theme(legend.position="bottom")
```


### Remove the blank, subset to top 50 ASVs, save the Phyloseq object to make things easier in the future
```
IS18_18S_ps_noBlank = subset_samples(ps_18S, X.SampleID != "IS18-BL-B02")

plot_bar(IS18_18S_ps_noBlank, "Type", fill = "Phylum", facet_grid = "Site") + theme(legend.position="bottom")

most_abundant_taxa_all = sort(taxa_sums(IS18_18S_ps_noBlank), TRUE)

top50 = 50
most_abundant_taxa_50 = sort(taxa_sums(IS18_18S_ps_noBlank), TRUE)[1:top50]
print(most_abundant_taxa_50)

IS18_18S_50_ps = prune_taxa(names(most_abundant_taxa_50), IS18_18S_ps_noBlank)

saveRDS(IS18_18S_ps_noBlank, here("PS_objects", "IS18_18S_ps_noBlank.rds"))
saveRDS(IS18_18S_50_ps, here("PS_objects", "IS18_18S_50_ps.rds"))

```

### How many seqs did we lose by subsetting?
```{r}
sum(colSums(otu_table(ps_18S)))

sum(colSums(otu_table(IS18_18S_50_ps)))
```


### View again as Top 50 ASVs (and no Blank)
```{r, warning=FALSE}
plot_bar(IS18_18S_50_ps, "Sample_sheet_name", fill = "Phylum", facet_grid = "Site") + theme(legend.position="bottom")

plot_bar(IS18_18S_50_ps, "Type", fill = "Phylum", facet_grid = "Site") + theme(legend.position="bottom")

```

### Let's ordinate our samples and see how they compare
```{r}
p_ord <- plot_ordination(IS18_18S_50_ps, ordinate(IS18_18S_50_ps, "NMDS", "bray"), type = "samples", color = "Site", shape = "Type") + geom_point(size = 3) 

p_ord + geom_text(check_overlap = TRUE, mapping = aes(label=Sample_sheet_name), size =3, vjust = 1.5) + theme_bw() #+ facet_wrap(AUS_ITS_ps)
```

### Now by samples and associated top level Phyla
```{r}
plot_ordination(IS18_18S_50_ps, ordinate(IS18_18S_50_ps, "NMDS", "bray"), type = "split", color = "Family", shape = "Type", label = "Sample_sheet_name") + geom_point(size = 3) #+ facet_wrap(AUS_ITS_ps)
```

### Lastly, a heat map ordered by site and resolved at the Class level
```{r}
plot_heatmap(AUS50_noEuc_ps, method = "NMDS", distance = "bray", sample.label = "Sample_sheet_name", sample.order ="Month",  taxa.label = "Class")
```



### Alpha diversity

plot_richness(ps, x="Type", measures=c("Shannon", "Simpson"), color="Site")


### Transform data to proportions as appropriate for Bray-Curtis distances

ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")

plot_ordination(ps.prop, ord.nmds.bray, color="Type", title="Bray NMDS")

### Bar plots

top20 <- names(sort(taxa_sums(ps), decreasing=TRUE)[1:20])
top50 <- names(sort(taxa_sums(ps), decreasing=TRUE)[1:50])

ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU)) # This should transform to relative abundance, but it doesn't seem to be working.

ps.top20 <- prune_taxa(top20, ps.top20)
ps.top50 <- prune_taxa(top50, ps)

# Full abudance bar plots
plot_bar(ps, x="Type", 
  fill="Phylum"
  ) 
  + facet_wrap(~Type, scales="free_x")

# Top 20 ASVs bar plots
plot_bar(ps.top20.tree, x="Type", 
  fill="Phylum"
  ) 
  + facet_wrap(~Type, scales="free_x")

# Top 50 ASVs bar plots
plot_bar(ps.top50, x="Type", 
  fill="Phylum"
  ) 
  + facet_wrap(~Type, scales="free_x")


# Top 20 w/ tree
ps.top20.tree <- prune_taxa(top20, ps)

plot(phy_tree(ps.top20.tree), show.node.label = TRUE)


plot_tree(ps.top20.tree, color = "Type", label.tips = "Phylum", ladderize = "left", justify = "left" , size = "Abundance")

# Looking at JUST the algae - subset the data
ps.chlorophyta = subset_taxa(ps, Phylum=="Chlorophyta_ph")

# Subset further, only keeping taxa with >

plot_bar(ps.chlorophyta, x="Type", 
  fill="Genus"
  ) 
  + facet_wrap(~Type, scales="free_x")